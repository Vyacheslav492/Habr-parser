import requests
from bs4 import BeautifulSoup
import sqlite3
import logging
import hashlib
import os

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

DB_PATH = r"D:\Programm and code\Python\Project in progress\Habr_paper_pars\Habr_paper_pars2.db"
BASE_URL = "https://habr.com/ru/hubs/python/articles/"

def create_database():
    # –°–æ–∑–¥–∞—ë–º –ø–∞–ø–∫—É, –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
    os.makedirs(os.path.dirname(DB_PATH), exist_ok=True)

    db_exists = os.path.exists(DB_PATH)
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS articles (
            id TEXT PRIMARY KEY,
            title TEXT,
            description TEXT,
            link TEXT,
            article_number INTEGER,
            page_number INTEGER
        )
    """)
    conn.commit()
    conn.close()
    if db_exists:
        logging.info(f"‚úÖ –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö '{DB_PATH}' —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, —Ç–∞–±–ª–∏—Ü–∞ 'articles' –ø—Ä–æ–≤–µ—Ä–µ–Ω–∞")
    else:
        logging.info(f"‚úÖ –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö '{DB_PATH}' —Å–æ–∑–¥–∞–Ω–∞ –∏ —Ç–∞–±–ª–∏—Ü–∞ 'articles' –≥–æ—Ç–æ–≤–∞")

def get_html(url):
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}
    try:
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        return response.text
    except requests.RequestException as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ HTML: {e}")
        return None

def extract_articles(html, page_number):
    soup = BeautifulSoup(html, "html.parser")
    articles_data = []

    articles = soup.find_all("article")
    if not articles:
        logging.warning("‚ö†Ô∏è –°—Ç–∞—Ç—å–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        return articles_data

    for idx, article in enumerate(articles, start=1):
        # –ù–∞–∑–≤–∞–Ω–∏–µ –∏ —Å—Å—ã–ª–∫–∞
        title_tag = article.find("h2")
        if title_tag:
            a_tag = title_tag.find("a", href=True)
            title = a_tag.get_text(strip=True)
            link = "https://habr.com" + a_tag['href']
        else:
            title = "–ù–∞–∑–≤–∞–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"
            link = "#"

        # –ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
        desc_tag = article.find("div", class_="tm-article-body tm-article-snippet__lead")
        if not desc_tag:
            desc_tag = article.find("div", class_="article-formatted-body")
        description = desc_tag.get_text(strip=True) if desc_tag else ""

        # –£–Ω–∏–∫–∞–ª—å–Ω—ã–π ID
        uid = hashlib.md5(link.encode("utf-8")).hexdigest()

        articles_data.append((uid, title, description, link, idx, page_number))

    logging.info(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(articles_data)} —Å—Ç–∞—Ç–µ–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_number}")
    return articles_data

def insert_articles(data):
    if not data:
        logging.warning("‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏")
        return

    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    try:
        cursor.executemany(
            """
            INSERT OR IGNORE INTO articles 
            (id, title, description, link, article_number, page_number) 
            VALUES (?, ?, ?, ?, ?, ?)
            """,
            data
        )
        conn.commit()
        logging.info(f"‚úÖ –í—Å—Ç–∞–≤–ª–µ–Ω–æ {cursor.rowcount} –Ω–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π –≤ –ë–î")
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—Å—Ç–∞–≤–∫–µ –¥–∞–Ω–Ω—ã—Ö: {e}")
    finally:
        conn.close()

def show_sample(n=5):
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    try:
        cursor.execute("SELECT * FROM articles LIMIT ?", (n,))
        rows = cursor.fetchall()
        logging.info(f"üìä –ü–µ—Ä–≤—ã–µ {n} –∑–∞–ø–∏—Å–µ–π –≤ –±–∞–∑–µ:")
        for row in rows:
            print(row)
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –±–∞–∑—ã: {e}")
    finally:
        conn.close()

def main():
    create_database()
    all_data = []

    for page in range(1, 6):  # –ø–µ—Ä–≤—ã–µ 5 —Å—Ç—Ä–∞–Ω–∏—Ü
        url = BASE_URL if page == 1 else f"{BASE_URL}page{page}/"
        logging.info(f"üìÑ –°–∫–∞—á–∏–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É {page}: {url}")
        html = get_html(url)
        if not html:
            logging.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É {page}")
            continue

        page_data = extract_articles(html, page_number=page)
        all_data.extend(page_data)

    insert_articles(all_data)
    show_sample()
    logging.info("üéâ –ü–∞—Ä—Å–∏–Ω–≥ Habr Python-—Ö–∞–±–∞ –∑–∞–≤–µ—Ä—à—ë–Ω —É—Å–ø–µ—à–Ω–æ")

if __name__ == "__main__":
    main()
